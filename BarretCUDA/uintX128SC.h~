// This file was generated by BarretCUDA v0.1 
// 
// BarretCUDA is a fast(ish) CUDA implementation of sparse matrix
// multiplication modulo a multi-precision prime.
// 
// Copyright (C) 2016, Ryan Henry and Syed Mahbub Hafiz
// 
// 
// BarretCUDA is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published
// by the Free Software Foundation, either version 3 of the License,
// or (at your option) any later version.
// 
// BarretCUDA is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
// 
// You should have received a copy of the GNU General Public License
// along with BarretCUDA.  If not, see <http://www.gnu.org/licenses/>.

#ifndef __UINT_128_H
#define __UINT_128_H
#include "uint.h"

#ifndef __UINTX__
#define __UINTX__
    typedef uint128 uintX;
#endif

static inline uint128 make_uint128(const uint w0x, const uint w0y, const uint w0z, const uint w0w)
{
    uint128 res;

    uint * _res = (uint *)&res;
    _res[0]= w0x;				// res.w0.x= w0x
    _res[1]= w0y;				// res.w0.y= w0y
    _res[2]= w0z;				// res.w0.z= w0z
    _res[3]= w0w;				// res.w0.w= w0w

    return res;
}

static inline NTL::ZZ to_ZZ(const uint128 & n)
{
    return to_ZZ<uint128>(n);
}

static inline NTL::ZZ_p to_ZZ_p(const uint128 & n)
{
    return NTL::to_ZZ_p(to_ZZ<uint128>(n));
}

static inline uint128 to_uint128(const NTL::ZZ & n)
{
    return to_uint<uint128>(n);
}

__device__ __forceinline__ void normalize(const uint128 & aLo, const uint128 & aHi, const uintX<uint128> & s)
{
    uint * _aLo = (uint *)&aLo;
    uint * _aHi = (uint *)&aHi;
    const uint * _s = (uint *)&s;
    asm("sub.cc.u32	 %0, %0, %8;\n\t"	// r0-= r8
	"subc.cc.u32	 %1, %1, %9;\n\t"	// r1-=( r9+c)
	"subc.cc.u32	 %2, %2,%10;\n\t"	// r2-=(r10+c)
	"subc.cc.u32	 %3, %3,%11;\n\t"	// r3-=(r11+c)
	"subc.cc.u32	 %4, %4,%12;\n\t"	// r4-=(r12+c)
	"subc.cc.u32	 %5, %5,%13;\n\t"	// r5-=(r13+c)
	"subc.cc.u32	 %6, %6,%14;\n\t"	// r6-=(r14+c)
	"subc.u32	 %7, %7,%15;\n\t"	// r7-=(r15+c)
	: "+r"(_aLo[0]), "+r"(_aLo[1]), "+r"(_aLo[2]), "+r"(_aLo[3]), "+r"(_aHi[0]), "+r"(_aHi[1]), "+r"(_aHi[2]), "+r"(_aHi[3])
	: "r"(_s[0]), "r"(_s[1]), "r"(_s[2]), "r"(_s[3]), "r"(_s[4]), "r"(_s[5]), "r"(_s[6]), "r"(_s[7]));
}

__device__ __forceinline__ void sub(const uint128 & aLo, const uint & aHi, const uintXp<uint128> & r)
{
    uint * _aLo = (uint *)&aLo;
    uint * _aHi = (uint *)&aHi;
    const uint * _r = (uint *)&r;
    asm("sub.cc.u32	 %0, %0, %5;\n\t"	// r0-= r5
	"subc.cc.u32	 %1, %1, %6;\n\t"	// r1-=( r6+c)
	"subc.cc.u32	 %2, %2, %7;\n\t"	// r2-=( r7+c)
	"subc.cc.u32	 %3, %3, %8;\n\t"	// r3-=( r8+c)
	"subc.u32	 %4, %4, %9;\n\t"	// r4-=( r9+c)
	: "+r"(_aLo[0]), "+r"(_aLo[1]), "+r"(_aLo[2]), "+r"(_aLo[3]), "+r"(_aHi[0])
	: "r"(_r[0]), "r"(_r[1]), "r"(_r[2]), "r"(_r[3]), "r"(_r[4]));
}

__device__ __forceinline__ uintXp<uint128> get_q(const uint & aLo, const uint128 & aHi, const uint128 & mu)
{
    uint __attribute__((unused)) tmp;
    uintXp<uint128> q;
    uint * _q = (uint *)&q;
    uint * _aLo = (uint *)&aLo;
    uint * _aHi = (uint *)&aHi;
    uint * _mu = (uint *)&mu;
    asm("mul.hi.u32     %0, %6, %11;\n\t"               //(O0 = r6  * s11.hi)
"mad.lo.cc.u32  %0, %7, %11, %0;\n\t"           //(O0 += r7 * s11.lo)
"madc.hi.u32    %1, %7, %11, 0;\n\t"            //(O1 = r7 * s11.hi + c  )

"mad.lo.cc.u32  %0, %6, %12, %0;\n\t"           //(O0 += r6 * s12.lo)
"madc.hi.cc.u32 %1, %6, %12, %1;\n\t"           //(O1 += r6 * s12.hi + c  )
"madc.lo.u32    %2, %9, %11, 0;\n\t"            //(O2 = r9 * s11.lo + c  )

"mad.lo.cc.u32  %1, %8, %11, %1;\n\t"           //(O1 += r8 * s11.lo)
"madc.hi.cc.u32 %2, %8, %11, %2;\n\t"           //(O2 += r8 * s11.hi + c  )
"madc.lo.u32    %3, %10, %11, 0;\n\t"           //(O3 = r10 * s11.lo + c  )

"mad.lo.cc.u32  %1, %7, %12, %1;\n\t"           //(O1 += r7 * s12.lo)
"madc.hi.cc.u32 %2, %7, %12, %2;\n\t"           //(O2 += r7 * s12.hi + c  )
"madc.lo.cc.u32 %3, %9, %12, %3;\n\t"           //(O3 += r9 * s12.lo + c  )
"madc.hi.u32    %4, %10, %11, 0;\n\t"           //(O4 = r10 * s11.hi + c  )

"mad.lo.cc.u32  %1, %6, %13, %1;\n\t"           //(O1 += r6 * s13.lo)
"madc.hi.cc.u32 %2, %6, %13, %2;\n\t"           //(O2 += r6 * s13.hi + c  )
"madc.lo.cc.u32 %3, %8, %13, %3;\n\t"           //(O3 += r8 * s13.lo + c  )
"madc.hi.cc.u32 %4, %9, %12, %4;\n\t"           //(O4 += r9 * s12.hi + c  )
"madc.lo.u32    %5, %10, %13, 0;\n\t"           //(O5 = r10 * s13.lo + c  )

"mad.lo.cc.u32  %2, %8, %12, %2;\n\t"           //(O2 += r8 * s12.lo)
"madc.hi.cc.u32 %3, %9, %11, %3;\n\t"           //(O3 += r9 * s11.hi + c  )
"madc.lo.cc.u32 %4, %10, %12, %4;\n\t"          //(O4 += r10 * s12.lo + c  )
"madc.hi.cc.u32 %5, %10, %12, %5;\n\t"          //(O5 += r10 * s12.hi + c  )
"madc.lo.u32    %0, %10, %14, 0;\n\t"           //(O0 = r10 * s14.lo + c  )

"mad.lo.cc.u32  %2, %7, %13, %2;\n\t"           //(O2 += r7 * s13.lo)
"madc.hi.cc.u32 %3, %8, %12, %3;\n\t"           //(O3 += r8 * s12.hi + c  )
"madc.lo.cc.u32 %4, %9, %13, %4;\n\t"           //(O4 += r9 * s13.lo + c  )
"madc.hi.cc.u32 %5, %9, %13, %5;\n\t"           //(O5 += r9 * s13.hi + c  )
"addc.cc.u32    %0, %9, %0;\n\t"                //(O0 += r9 + c)
"madc.hi.u32    %1, %10, %14, 0;\n\t"           //(O1 = r10 * s14.hi + c  )

"mad.lo.cc.u32  %2, %6, %14, %2;\n\t"           //(O2 += r6 * s14.lo)
"madc.hi.cc.u32 %3, %7, %13, %3;\n\t"           //(O3 += r7 * s13.hi + c  )
"madc.lo.cc.u32 %4, %8, %14, %4;\n\t"           //(O4 += r8 * s14.lo + c  )
"madc.hi.cc.u32 %5, %8, %14, %5;\n\t"           //(O5 += r8 * s14.hi + c  )
"madc.hi.cc.u32 %0, %10, %13, %0;\n\t"          //(O0 += r10 * s13.hi + c  )
"addc.cc.u32    %1, %10, %1;\n\t"               //(O1 += r10 + c)
"addc.u32       %2, 0, 0;\n\t"          //(O2 = +c)

"mad.lo.cc.u32  %3, %7, %14, %3;\n\t"           //(O3 += r7 * s14.lo)
"madc.hi.cc.u32 %4, %8, %13, %4;\n\t"           //(O4 += r8 * s13.hi + c  )
"madc.lo.cc.u32 %5, %9, %14, %5;\n\t"           //(O5 += r9 * s14.lo + c  )
"madc.hi.cc.u32 %0, %9, %14, %0;\n\t"           //(O0 += r9 * s14.hi + c  )
"addc.cc.u32    %1, 0, %1;\n\t"         //(O1  + = +c)
"addc.u32       %2, 0, %2;\n\t"         //(O2 + = +c)

"add.cc.u32     %3, %6, %3;\n\t"                //(O3 += r6)
"madc.hi.cc.u32 %4, %7, %14, %4;\n\t"           //(O4 += r7 * s14.hi + c  )
"addc.cc.u32    %5, %8, %5;\n\t"                //(O5 += r8 + c)
"addc.cc.u32    %0, 0, %0;\n\t"         //(O0  + = +c)
"addc.cc.u32    %1, 0, %1;\n\t"         //(O1  + = +c)
"addc.u32       %2, 0, %2;\n\t"         //(O2 + = +c)

"add.cc.u32     %4, %7, %4;\n\t"                //(O4 += r7)
"addc.cc.u32    %5, 0, %5;\n\t"         //(O5  + = +c)
"addc.cc.u32    %0, 0, %0;\n\t"         //(O0  + = +c)
"addc.cc.u32    %1, 0, %1;\n\t"         //(O1  + = +c)
"addc.u32       %2, 0, %2;\n\t"         //(O2 + = +c)
	: "=r"(_q[2]), "=r"(_q[3]), "=r"(_q[4]), "=r"(tmp), "=r"(_q[0]), "=r"(_q[1])
	: "r"(_aLo[0]), "r"(_aHi[0]), "r"(_aHi[1]), "r"(_aHi[2]), "r"(_aHi[3]), "r"(_mu[0]), "r"(_mu[1]), "r"(_mu[2]), "r"(_mu[3]));

    return q;
}

/*__device__ __forceinline__ uintXp<uint128> get_q(const uint & aLo, const uint128 & aHi, const uint128 & mu)
{
    uint __attribute__((unused)) tmp;
    uintXp<uint128> q;
    uint * _q = (uint *)&q;
    uint * _aLo = (uint *)&aLo;
    uint * _aHi = (uint *)&aHi;
    uint * _mu = (uint *)&mu;
    asm("mul.hi.u32	 %2, %6,%11    ;\n\t"	// r2 =[ r6*r11].hi    (r-3 => r2)
	"mad.lo.cc.u32	 %2, %7,%11, %2;\n\t"	// r2+=[ r7*r11].lo    (r-3 => r2)
	"madc.lo.u32	 %3, %8,%11,  0;\n\t"	// r3 =[ r8*r11].lo+c  (r-2 => r3)
	"mad.lo.cc.u32	 %2, %6,%12, %2;\n\t"	// r2+=[ r6*r12].lo    (r-3 => r2)
	"madc.lo.cc.u32	 %3, %7,%12, %3;\n\t"	// r3+=[ r7*r12].lo+c  (r-2 => r3)
	"madc.lo.u32	 %4, %9,%11,  0;\n\t"	// r4 =[ r9*r11].lo+c  (r-1 => r4)
	"mad.lo.cc.u32	 %3, %6,%13, %3;\n\t"	// r3+=[ r6*r13].lo    (r-2 => r3)
	"madc.lo.cc.u32	 %4, %8,%12, %4;\n\t"	// r4+=[ r8*r12].lo+c  (r-1 => r4)
	"madc.lo.u32	 %0,%10,%11,  0;\n\t"	// r0 =[r10*r11].lo+c
	"mad.hi.cc.u32	 %3, %7,%11, %3;\n\t"	// r3+=[ r7*r11].hi    (r-2 => r3)
	"madc.lo.cc.u32	 %4, %7,%13, %4;\n\t"	// r4+=[ r7*r13].lo+c  (r-1 => r4)
	"madc.lo.cc.u32	 %0, %9,%12, %0;\n\t"	// r0+=[ r9*r12].lo+c
	"madc.lo.u32	 %1,%10,%12,  0;\n\t"	// r1 =[r10*r12].lo+c
	"mad.hi.cc.u32	 %3, %6,%12, %3;\n\t"	// r3+=[ r6*r12].hi    (r-2 => r3)
	"madc.lo.cc.u32	 %4, %6,%14, %4;\n\t"	// r4+=[ r6*r14].lo+c  (r-1 => r4)
	"madc.lo.cc.u32	 %0, %8,%13, %0;\n\t"	// r0+=[ r8*r13].lo+c
	"madc.lo.cc.u32	 %1, %9,%13, %1;\n\t"	// r1+=[ r9*r13].lo+c
	"madc.lo.u32	 %2,%10,%13,  0;\n\t"	// r2 =[r10*r13].lo+c
	"mad.hi.cc.u32	 %4, %8,%11, %4;\n\t"	// r4+=[ r8*r11].hi    (r-1 => r4)
	"madc.lo.cc.u32	 %0, %7,%14, %0;\n\t"	// r0+=[ r7*r14].lo+c
	"madc.lo.cc.u32	 %1, %8,%14, %1;\n\t"	// r1+=[ r8*r14].lo+c
	"madc.lo.cc.u32	 %2, %9,%14, %2;\n\t"	// r2+=[ r9*r14].lo+c
	"madc.lo.u32	 %3,%10,%14,  0;\n\t"	// r3 =[r10*r14].lo+c
	"mad.hi.cc.u32	 %4, %7,%12, %4;\n\t"	// r4+=[ r7*r12].hi    (r-1 => r4)
	"madc.hi.cc.u32	 %0, %9,%11, %0;\n\t"	// r0+=[ r9*r11].hi+c
	"madc.hi.cc.u32	 %1,%10,%11, %1;\n\t"	// r1+=[r10*r11].hi+c
	"madc.hi.cc.u32	 %2,%10,%12, %2;\n\t"	// r2+=[r10*r12].hi+c
	"madc.hi.cc.u32	 %3,%10,%13, %3;\n\t"	// r3+=[r10*r13].hi+c
	"madc.hi.u32	 %4,%10,%14,  0;\n\t"	// r4 =[r10*r14].hi+c
	"mad.hi.cc.u32	 %4, %6,%13, %4;\n\t"	// r4+=[ r6*r13].hi    (r-1 => r4)
	"madc.hi.cc.u32	 %0, %8,%12, %0;\n\t"	// r0+=[ r8*r12].hi+c
	"madc.hi.cc.u32	 %1, %9,%12, %1;\n\t"	// r1+=[ r9*r12].hi+c
	"madc.hi.cc.u32	 %2, %9,%13, %2;\n\t"	// r2+=[ r9*r13].hi+c
	"madc.hi.cc.u32	 %3, %9,%14, %3;\n\t"	// r3+=[ r9*r14].hi+c
	"addc.cc.u32	 %4, %4,  0    ;\n\t"	// r4+= c
	"addc.u32	 %5,  0,  0    ;\n\t"	// r5 = c
	"mad.hi.cc.u32	 %0, %7,%13, %0;\n\t"	// r0+=[ r7*r13].hi  
	"madc.hi.cc.u32	 %1, %8,%13, %1;\n\t"	// r1+=[ r8*r13].hi+c
	"madc.hi.cc.u32	 %2, %8,%14, %2;\n\t"	// r2+=[ r8*r14].hi+c
	"addc.cc.u32	 %3, %3,  0    ;\n\t"	// r3+= c
	"addc.cc.u32	 %4, %4,  0    ;\n\t"	// r4+= c
	"addc.u32	 %5, %5,  0    ;\n\t"	// r5+= c
	"mad.hi.cc.u32	 %0, %6,%14, %0;\n\t"	// r0+=[ r6*r14].hi  
	"madc.hi.cc.u32	 %1, %7,%14, %1;\n\t"	// r1+=[ r7*r14].hi+c
	"addc.cc.u32	 %2, %2,  0    ;\n\t"	// r2+= c
	"addc.cc.u32	 %3, %3,  0    ;\n\t"	// r3+= c
	"addc.cc.u32	 %4, %4,  0    ;\n\t"	// r4+= c
	"addc.u32	 %5, %5,  0    ;\n\t"	// r5+= c
	"add.cc.u32	 %0, %0, %7    ;\n\t"	// r0+= r7
	"addc.cc.u32	 %1, %1, %8    ;\n\t"	// r1+= r8+c
	"addc.cc.u32	 %2, %2, %9    ;\n\t"	// r2+= r9+c
	"addc.cc.u32	 %3, %3,%10    ;\n\t"	// r3+=r10+c
	"addc.cc.u32	 %4, %4,%11    ;\n\t"	// r4+=r11+c
	"addc.u32	 %5, %5,  0    ;\n\t"	// r5+=c
	: "=r"(tmp), "=r"(_q[0]), "=r"(_q[1]), "=r"(_q[2]), "=r"(_q[3]), "=r"(_q[4])
	: "r"(_aLo[0]), "r"(_aHi[0]), "r"(_aHi[1]), "r"(_aHi[2]), "r"(_aHi[3]), "r"(_mu[0]), "r"(_mu[1]), "r"(_mu[2]), "r"(_mu[3]));

    return q;
}*/

__device__ __forceinline__ uintXp<uint128> get_r2(const uintXp<uint128> & q, const uint128 & modulus)
{
    uintXp<uint128> r;
    uint * _r = (uint *)&r;
    uint * _q = (uint *)&q;
    uint * _m = (uint *)&modulus;

    asm("mad.lo.u32	 %0, %5,%10,  0;\n\t"	// r0 =[ r5*r10].lo  
	"mad.lo.u32	 %1, %5,%11,  0;\n\t"	// r1 =[ r5*r11].lo  
	"mad.lo.cc.u32	 %1, %6,%10, %1;\n\t"	// r1+=[ r6*r10].lo  
	"madc.lo.u32	 %2, %5,%12, %2;\n\t"	// r2+=[ r5*r12].lo+c
	"mad.hi.cc.u32	 %1, %5,%10, %1;\n\t"	// r1+=[ r5*r10].hi  
	"madc.lo.cc.u32	 %2, %6,%11, %2;\n\t"	// r2+=[ r6*r11].lo+c
	"madc.lo.u32	 %3, %5,%13,  0;\n\t"	// r3 =[ r5*r13].lo+c
	"mad.hi.cc.u32	 %2, %5,%11, %2;\n\t"	// r2+=[ r5*r11].hi  
	"madc.lo.cc.u32	 %3, %6,%12, %3;\n\t"	// r3+=[ r6*r12].lo+c
	"madc.lo.u32	 %4, %6,%13,  0;\n\t"	// r4 =[ r6*r13].lo+c
	"mad.lo.cc.u32	 %2, %7,%10, %2;\n\t"	// r2+=[ r7*r10].lo  
	"madc.hi.cc.u32	 %3, %5,%12, %3;\n\t"	// r3+=[ r5*r12].hi+c
	"madc.hi.cc.u32	 %4, %5,%13, %4;\n\t"	// r4+=[ r5*r13].hi+c
	"mad.hi.cc.u32	 %2, %6,%10, %2;\n\t"	// r2+=[ r6*r10].hi  
	"madc.lo.cc.u32	 %3, %7,%11, %3;\n\t"	// r3+=[ r7*r11].lo+c
	"madc.lo.cc.u32	 %4, %7,%12, %4;\n\t"	// r4+=[ r7*r12].lo+c
	"mad.hi.cc.u32	 %3, %6,%11, %3;\n\t"	// r3+=[ r6*r11].hi  
	"madc.hi.cc.u32	 %4, %6,%12, %4;\n\t"	// r4+=[ r6*r12].hi+c
	"mad.lo.cc.u32	 %3, %8,%10, %3;\n\t"	// r3+=[ r8*r10].lo  
	"madc.lo.cc.u32	 %4, %8,%11, %4;\n\t"	// r4+=[ r8*r11].lo+c
	"mad.hi.cc.u32	 %3, %7,%10, %3;\n\t"	// r3+=[ r7*r10].hi  
	"madc.hi.cc.u32	 %4, %7,%11, %4;\n\t"	// r4+=[ r7*r11].hi+c
	"mad.lo.cc.u32	 %4, %9,%10, %4;\n\t"	// r4+=[ r9*r10].lo  
	"mad.hi.cc.u32	 %4, %8,%10, %4;\n\t"	// r4+=[ r8*r10].hi  
	: "=r"(_r[0]), "=r"(_r[1]), "=r"(_r[2]), "=r"(_r[3]), "=r"(_r[4])
	: "r"(_q[0]), "r"(_q[1]), "r"(_q[2]), "r"(_q[3]), "r"(_q[4]), "r"(_m[0]), "r"(_m[1]), "r"(_m[2]), "r"(_m[3]));

    return r;
}
__device__ __forceinline__ void mad(const uint128 & aLo, const uint128 & aHi, uint & overflow, const uint128 & b, const uint128 & c)
{
    uint * _aLo = (uint *)&aLo;
    uint * _aHi = (uint *)&aHi;
    const uint * _b = (uint *)&b;
    const uint * _c = (uint *)&c;

    asm("mad.lo.cc.u32	 %0, %9,%13, %0;\n\t"	// r0+=[ r9*r13].lo  
	"madc.hi.cc.u32	 %1, %9,%13, %1;\n\t"	// r1+=[ r9*r13].hi  
	"madc.lo.cc.u32	 %2,%11,%13, %2;\n\t"	// r2+=[r11*r13].lo+c
	"madc.hi.cc.u32	 %3,%11,%13, %3;\n\t"	// r3+=[r11*r13].hi+c
	"madc.lo.cc.u32	 %4,%12,%14, %4;\n\t"	// r4+=[r12*r14].lo+c
	"madc.hi.cc.u32	 %5,%12,%14, %5;\n\t"	// r5+=[r12*r14].hi+c
	"madc.lo.cc.u32	 %6,%12,%16, %6;\n\t"	// r6+=[r12*r16].lo+c
	"madc.hi.cc.u32	 %7,%12,%16, %7;\n\t"	// r7+=[r12*r16].hi+c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %1,%10,%13, %1;\n\t"	// r1+=[r10*r13].lo  
	"madc.hi.cc.u32	 %2,%10,%13, %2;\n\t"	// r2+=[r10*r13].hi  
	"madc.lo.cc.u32	 %3,%12,%13, %3;\n\t"	// r3+=[r12*r13].lo+c
	"madc.hi.cc.u32	 %4,%12,%13, %4;\n\t"	// r4+=[r12*r13].hi+c
	"madc.lo.cc.u32	 %5,%12,%15, %5;\n\t"	// r5+=[r12*r15].lo+c
	"madc.hi.cc.u32	 %6,%12,%15, %6;\n\t"	// r6+=[r12*r15].hi+c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %1, %9,%14, %1;\n\t"	// r1+=[ r9*r14].lo  
	"madc.hi.cc.u32	 %2, %9,%14, %2;\n\t"	// r2+=[ r9*r14].hi  
	"madc.lo.cc.u32	 %3,%11,%14, %3;\n\t"	// r3+=[r11*r14].lo+c
	"madc.hi.cc.u32	 %4,%11,%14, %4;\n\t"	// r4+=[r11*r14].hi+c
	"madc.lo.cc.u32	 %5,%11,%16, %5;\n\t"	// r5+=[r11*r16].lo+c
	"madc.hi.cc.u32	 %6,%11,%16, %6;\n\t"	// r6+=[r11*r16].hi+c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %2,%10,%14, %2;\n\t"	// r2+=[r10*r14].lo  
	"madc.hi.cc.u32	 %3,%10,%14, %3;\n\t"	// r3+=[r10*r14].hi  
	"madc.lo.cc.u32	 %4,%11,%15, %4;\n\t"	// r4+=[r11*r15].lo+c
	"madc.hi.cc.u32	 %5,%11,%15, %5;\n\t"	// r5+=[r11*r15].hi+c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %2, %9,%15, %2;\n\t"	// r2+=[ r9*r15].lo  
	"madc.hi.cc.u32	 %3, %9,%15, %3;\n\t"	// r3+=[ r9*r15].hi  
	"madc.lo.cc.u32	 %4,%10,%16, %4;\n\t"	// r4+=[r10*r16].lo+c
	"madc.hi.cc.u32	 %5,%10,%16, %5;\n\t"	// r5+=[r10*r16].hi+c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %3,%10,%15, %3;\n\t"	// r3+=[r10*r15].lo  
	"madc.hi.cc.u32	 %4,%10,%15, %4;\n\t"	// r4+=[r10*r15].hi  
	"addc.cc.u32	 %5, %5,  0    ;\n\t"	// r5+=c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %3, %9,%16, %3;\n\t"	// r3+=[ r9*r16].lo  
	"madc.hi.cc.u32	 %4, %9,%16, %4;\n\t"	// r4+=[ r9*r16].hi  
	"addc.cc.u32	 %5, %5,  0    ;\n\t"	// r5+=c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	: "+r"(_aLo[0]), "+r"(_aLo[1]), "+r"(_aLo[2]), "+r"(_aLo[3]), "+r"(_aHi[0]), "+r"(_aHi[1]), "+r"(_aHi[2]), "+r"(_aHi[3]), "+r"(overflow)
	: "r"(_b[0]), "r"(_b[1]), "r"(_b[2]), "r"(_b[3]), "r"(_c[0]), "r"(_c[1]), "r"(_c[2]), "r"(_c[3]));
}

#endif

